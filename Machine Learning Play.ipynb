{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68116bd1",
   "metadata": {},
   "source": [
    "# Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ec41c9",
   "metadata": {},
   "source": [
    "Machine learning can be branched out into the following categories:\n",
    "\n",
    "    Supervised Learning\n",
    "    Unsupervised Learning\n",
    "    \n",
    "We are using ML to provide  predictive models to do good and enhance human life. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1989353b",
   "metadata": {},
   "source": [
    "### Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af9683f",
   "metadata": {},
   "source": [
    "##### Mean, Median, and Mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a8e1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "from scipy import stats # to allow mode to count\n",
    "\n",
    "speed = [99,86,87,88,111,86,103,87,94,78,77,85,86]\n",
    "\n",
    "x = numpy.mean(speed) # Mean\n",
    "print(x)\n",
    "x = numpy.median(speed) # Median\n",
    "print(x) \n",
    "\n",
    "x = stats.mode(speed)\n",
    "\n",
    "print(x) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd12d47e",
   "metadata": {},
   "source": [
    "##### Standard Deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d03e36ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "\n",
    "speed = [86,87,88,86,87,85,86]\n",
    "\n",
    "x = numpy.std(speed)\n",
    "\n",
    "print(x) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b3c743",
   "metadata": {},
   "source": [
    "##### Percentile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22827c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "\n",
    "ages = [5,31,43,48,50,41,7,11,15,39,80,82,32,2,8,6,25,36,27,61,31]\n",
    "\n",
    "x = numpy.percentile(ages, 75)\n",
    "\n",
    "print(x) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a89a34",
   "metadata": {},
   "source": [
    "## Supervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e19884a",
   "metadata": {},
   "source": [
    "Supervised learning problems can be grouped into regression and classification problems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f12dd8b4",
   "metadata": {},
   "source": [
    "### Regression:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a1060c",
   "metadata": {},
   "source": [
    "In regression problems, we are trying to predict a continuous-valued output. Examples are:\n",
    "\n",
    "    What is the housing price in New York?\n",
    "    What is the value of cryptocurrencies?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d44596a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of simple regression\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Load the data\n",
    "housing_data = pd.read_csv('housing_data.csv')\n",
    "X = housing_data[['Sq ft', 'Burglaries']]\n",
    "y = housing_data['Rent']\n",
    "\n",
    "# Create the model\n",
    "reg = LinearRegression()\n",
    "\n",
    "# Train the model\n",
    "reg.fit(X, y)\n",
    "\n",
    "square_footage = 950\n",
    "number_of_burglaries = 1\n",
    "\n",
    "y_pred = reg.predict(np.array([square_footage, number_of_burglaries]).reshape(1, 2))\n",
    "\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f2a8bd",
   "metadata": {},
   "source": [
    "The goal of a linear regression model is to find the slope and intercept pair that minimizes loss on average across all of the data.\n",
    "\n",
    "As we try to minimize loss, we take each parameter we are changing, and move it as long as we are decreasing loss. It’s like we are moving down a hill, and stop once we reach the bottom. The process by which we do this is called gradient descent. We move in the direction that decreases our loss the most. Gradient refers to the slope of the curve at any point. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1794296d",
   "metadata": {},
   "source": [
    "#### Simple Linear Regression\n",
    "#### Multiple Linear Regression\n",
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0261dc8a",
   "metadata": {},
   "source": [
    "##### Simple Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2650865",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.w3schools.com/python/python_ml_linear_regression.asp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "586d196a",
   "metadata": {},
   "source": [
    "#####  Polynomial Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c62427",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.w3schools.com/python/python_ml_polynomial_regression.asp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d3ff7b",
   "metadata": {},
   "source": [
    "##### Multiple Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8148b181",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.w3schools.com/python/python_ml_multiple_regression.asp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5df19ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "Also\n",
    "\n",
    "https://www.w3schools.com/python/python_ml_scale.asp\n",
    "https://www.w3schools.com/python/python_ml_train_test.asp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34aec5c0",
   "metadata": {},
   "source": [
    "##### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d683115c",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.w3schools.com/python/python_ml_logistic_regression.asp\n",
    "https://www.w3schools.com/python/python_ml_grid_search.asp\n",
    "https://www.w3schools.com/python/python_ml_preprocessing.asp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90cb4480",
   "metadata": {},
   "source": [
    "#### K-Nearest Neighbor Regressor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea6d30b",
   "metadata": {},
   "source": [
    "The K-Nearest Neighbor algorithm can be used for regression. Rather than returning a classification, it returns a number. Use scikit-learn to implement the KNeighborsRegressor class, which is very similar to KNeighborsClassifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "656c1759",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example\n",
    "from movies import movie_dataset, movie_ratings\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "regressor = KNeighborsRegressor(n_neighbors = 5, weights = \"distance\")\n",
    "regressor.fit(movie_dataset,movie_ratings)\n",
    "print(regressor.predict([[0.016, 0.300, 1.022], [0.0004092981, 0.283, 1.0112], [0.00687649, 0.235, 1.0112]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d722084",
   "metadata": {},
   "source": [
    "#### Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c84d2a",
   "metadata": {},
   "source": [
    "Decision trees are machine learning models that try to find patterns in the features of data points. The decision trees can be used for classification and regression tasks.\n",
    "\n",
    "https://www.w3schools.com/python/python_ml_decision_tree.asp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36759263",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import tree\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "df = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/car/car.data', names=['buying', 'maint', 'doors', 'persons', 'lug_boot', 'safety', 'accep'])\n",
    "df['accep'] = ~(df['accep']=='unacc') #1 is acceptable, 0 if not acceptable\n",
    "X = pd.get_dummies(df.iloc[:,0:6])\n",
    "y = df['accep']\n",
    "x_train, x_test, y_train, y_test = train_test_split(X,y, random_state=0, test_size=0.2)\n",
    "\n",
    "## 1. Create a decision tree and print the parameters\n",
    "dtree = DecisionTreeClassifier()\n",
    "print(f'Decision Tree parameters: {dtree.get_params()}')\n",
    "\n",
    "## 2. Fit decision tree on training set and print the depth of the tree\n",
    "dtree.fit(x_train, y_train)\n",
    "print(f'Decision tree depth: {dtree.get_depth()}')\n",
    "\n",
    "## 3. Predict on test data and accuracy of model on test set\n",
    "y_pred = dtree.predict(x_test)\n",
    "\n",
    "print(f'Test set accuracy: {dtree.score(x_test, y_test)}') # or accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c972d17",
   "metadata": {},
   "source": [
    "##### Hyperparameters "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae8a102",
   "metadata": {},
   "source": [
    "In this lesson you will learn about the different methods one can use to tune hyperparameters in machine learning models and how to implement them in Python. Specifically we will be diving deep into two methods: grid search (GridSearchCV) and random search (RandomizedSearchCV).\n",
    "\n",
    "To understand the implementation of different methods of hyperparameter tuning, we need to choose a dataset, a classification or regression problem we’d like to solve, and a machine learning model to solve it with. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3dcaf7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.codecademy.com/courses/intro-to-hyperparameter-tuning-with-python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb0075d5",
   "metadata": {},
   "source": [
    "### Classification:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae5d2809",
   "metadata": {},
   "source": [
    "In classification problems, we are trying to predict a discrete number of values. Examples are:\n",
    "\n",
    "    Is this a picture of a human or a picture of a cyborg?\n",
    "    Is this email spam?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd40006",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.w3schools.com/python/python_ml_auc_roc.asp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd704c3f",
   "metadata": {},
   "source": [
    "#### K Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f35878e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of classification\n",
    "import pandas as pd\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Load the data\n",
    "photo_id_times = pd.read_csv('photo_id_times.csv')\n",
    "\n",
    "# Separate the data into independent and dependent variables\n",
    "X = np.array(photo_id_times['Time to id photo']).reshape(-1, 1)\n",
    "y = photo_id_times['Class']\n",
    "\n",
    "# Create a model and fit it to the data\n",
    "neigh = KNeighborsClassifier(n_neighbors=3)\n",
    "neigh.fit(X, y)\n",
    "\n",
    "time_to_identify_picture = 4\n",
    "\n",
    "# Make a prediction based on how long it takes to identify a picture\n",
    "y_pred = neigh.predict(np.array(time_to_identify_picture).reshape(1, -1))\n",
    "\n",
    "if y_pred == 1:\n",
    "    print(\"We think you're a robot.\")\n",
    "else:\n",
    "    print(\"Welcome, human!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09041e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.w3schools.com/python/python_ml_knn.asp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8870eb8e",
   "metadata": {},
   "source": [
    "#### Evaluation Metrics for Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07856534",
   "metadata": {},
   "source": [
    "Different metrics are needed to evaluate your machine learning model. When creating a machine learning algorithm capable of making predictions, an important step in the process is to measure the model’s predictive power."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23618578",
   "metadata": {},
   "source": [
    "##### Confusion Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e665a647",
   "metadata": {},
   "source": [
    "We can pass the features of our evaluation set through the trained model and get an output list of the predictions our model makes. We then compare each of those predictions to the actual labels.\n",
    "\n",
    "One common way to visualize these values is in a confusion matrix. In a confusion matrix the predicted classes are represented as columns and the actual classes are represented as rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f22da8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "actual = [1, 0, 0, 1, 1, 1, 0, 1, 1, 1]\n",
    "predicted = [0, 1, 1, 1, 1, 0, 1, 0, 1, 0]\n",
    "\n",
    "true_positives = 0\n",
    "true_negatives = 0\n",
    "false_positives = 0\n",
    "false_negatives = 0\n",
    "\n",
    "for i in range(len(predicted)):\n",
    "  if actual[i] == 1 and predicted[i] == 1:\n",
    "    true_positives += 1\n",
    "  if actual[i] == 0 and predicted[i] == 0:\n",
    "    true_negatives += 1\n",
    "  if actual[i] == 0 and predicted[i] == 1:\n",
    "    false_positives += 1\n",
    "  if actual[i] == 1 and predicted[i] == 0:\n",
    "    false_negatives += 1\n",
    "\n",
    "print(true_positives, true_negatives, false_positives, false_negatives)\n",
    "\n",
    "conf_matrix = confusion_matrix(actual, predicted)\n",
    "\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b5d67b",
   "metadata": {},
   "source": [
    "Classifying a single point can result in a true positive (actual = 1, predicted = 1), a true negative (actual = 0, predicted = 0), a false positive (actual = 0, predicted = 1), or a false negative (actual = 1, predicted = 0). These values are often summarized in a confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8599dae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.w3schools.com/python/python_ml_confusion_matrix.asp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c660fa",
   "metadata": {},
   "source": [
    "##### Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c101036",
   "metadata": {},
   "source": [
    "One method for determining the effectiveness of a classification algorithm is by measuring its accuracy statistic. Accuracy is calculated by finding the total number of correctly classified predictions (true positives and true negatives) and dividing by the total number of predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "517d0955",
   "metadata": {},
   "outputs": [],
   "source": [
    "actual = [1, 0, 0, 1, 1, 1, 0, 1, 1, 1]\n",
    "predicted = [0, 1, 1, 1, 1, 0, 1, 0, 1, 0]\n",
    "\n",
    "true_positives = 0\n",
    "true_negatives = 0\n",
    "false_positives = 0\n",
    "false_negatives = 0\n",
    "\n",
    "for i in range(len(predicted)):\n",
    "  #True Positives\n",
    "  if actual[i] == 1 and predicted[i] == 1:\n",
    "    true_positives += 1\n",
    "  #True Negatives\n",
    "  if actual[i] == 0 and predicted[i] == 0:\n",
    "    true_negatives += 1 \n",
    "  #False Positives\n",
    "  if actual[i] == 0 and predicted[i] == 1:\n",
    "    false_positives += 1\n",
    "  #False Negatives\n",
    "  if actual[i] == 1 and predicted[i] == 0:\n",
    "    false_negatives += 1\n",
    "    \n",
    "accuracy = (true_positives + true_negatives) / len(predicted)\n",
    "\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c002878",
   "metadata": {},
   "source": [
    "Accuracy measures how many classifications your algorithm got correct out of every classification it made."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d732d4",
   "metadata": {},
   "source": [
    "##### Recall"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a0d7744",
   "metadata": {},
   "source": [
    "Accuracy can be a misleading statistic depending on our data. In this situation, a helpful statistic to consider is recall. Recall is the ratio of correct positive predictions classifications made by the model to all actual positives.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ab389a",
   "metadata": {},
   "outputs": [],
   "source": [
    "actual = [1, 0, 0, 1, 1, 1, 0, 1, 1, 1]\n",
    "predicted = [0, 1, 1, 1, 1, 0, 1, 0, 1, 0]\n",
    "\n",
    "true_positives = 0\n",
    "true_negatives = 0\n",
    "false_positives = 0\n",
    "false_negatives = 0\n",
    "\n",
    "for i in range(len(predicted)):\n",
    "  if actual[i] == 1 and predicted[i] == 1:\n",
    "    true_positives += 1\n",
    "  if actual[i] == 0 and predicted[i] == 0:\n",
    "    true_negatives += 1\n",
    "  if actual[i] == 0 and predicted[i] == 1:\n",
    "    false_positives += 1\n",
    "  if actual[i] == 1 and predicted[i] == 0:\n",
    "    false_negatives += 1\n",
    "\n",
    "recall = true_positives/(true_positives + false_negatives)\n",
    "\n",
    "print(recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c758b2",
   "metadata": {},
   "source": [
    "Recall is the ratio of correct positive predictions classifications made by the model to all actual positives."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a5bc22",
   "metadata": {},
   "source": [
    "##### Percision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e550747d",
   "metadata": {},
   "source": [
    "Unfortunately, recall isn’t a perfect statistic either (spoiler alert! There is no perfect statistic). In this situation, a helpful statistic to understand is precision. Precision is the ratio of correct positive classifications to all positive classifications made by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b846b295",
   "metadata": {},
   "outputs": [],
   "source": [
    "actual = [1, 0, 0, 1, 1, 1, 0, 1, 1, 1]\n",
    "predicted = [0, 1, 1, 1, 1, 0, 1, 0, 1, 0]\n",
    "\n",
    "true_positives = 0\n",
    "true_negatives = 0\n",
    "false_positives = 0\n",
    "false_negatives = 0\n",
    "\n",
    "for i in range(len(predicted)):\n",
    "  if actual[i] == 1 and predicted[i] == 1:\n",
    "    true_positives += 1\n",
    "  if actual[i] == 0 and predicted[i] == 0:\n",
    "    true_negatives += 1\n",
    "  if actual[i] == 0 and predicted[i] == 1:\n",
    "    false_positives += 1\n",
    "  if actual[i] == 1 and predicted[i] == 0:\n",
    "    false_negatives += 1\n",
    "\n",
    "precision = true_positives/(true_positives + false_positives)\n",
    "\n",
    "print(precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b445d0",
   "metadata": {},
   "source": [
    "Precision is the ratio of correct positive classifications to all positive classifications made by the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e50c9692",
   "metadata": {},
   "source": [
    "##### F1-Score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6fd488",
   "metadata": {},
   "source": [
    "It is often useful to consider both the precision and recall when attempting to describe the effectiveness of a model. The F1-score combines both precision and recall into a single statistic, by determining their harmonic mean. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83754621",
   "metadata": {},
   "outputs": [],
   "source": [
    "actual = [1, 0, 0, 1, 1, 1, 0, 1, 1, 1]\n",
    "predicted = [0, 1, 1, 1, 1, 0, 1, 0, 1, 0]\n",
    "\n",
    "true_positives = 0\n",
    "true_negatives = 0\n",
    "false_positives = 0\n",
    "false_negatives = 0\n",
    "\n",
    "for i in range(len(predicted)):\n",
    "  if actual[i] == 1 and predicted[i] == 1:\n",
    "    true_positives += 1\n",
    "  if actual[i] == 0 and predicted[i] == 0:\n",
    "    true_negatives += 1\n",
    "  if actual[i] == 0 and predicted[i] == 1:\n",
    "    false_positives += 1\n",
    "  if actual[i] == 1 and predicted[i] == 0:\n",
    "    false_negatives += 1\n",
    "\n",
    "precision = true_positives / (true_positives + false_positives)\n",
    "recall = true_positives / (true_positives + false_negatives)\n",
    "\n",
    "f_1 = 2*precision*recall/(precision+recall)\n",
    "\n",
    "print(f_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "198b7744",
   "metadata": {},
   "source": [
    "F1-score is a combination of precision and recall. F1-score will be low if either precision or recall is low."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "796c66fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Review\n",
    "\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
    "\n",
    "actual = [1, 0, 0, 1, 1, 1, 0, 1, 1, 1]\n",
    "predicted = [0, 1, 1, 1, 1, 0, 1, 0, 1, 0]\n",
    "\n",
    "print(accuracy_score(actual, predicted))\n",
    "\n",
    "print(recall_score(actual, predicted))\n",
    "\n",
    "print(precision_score(actual, predicted))\n",
    "\n",
    "print(f1_score(actual,predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed852b70",
   "metadata": {},
   "source": [
    "As long as you have an understanding of what question you’re trying to answer, you should be able to determine which statistic is most relevant to you.\n",
    "\n",
    "The Python library scikit-learn has some functions that will calculate these statistics for you!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e79055",
   "metadata": {},
   "source": [
    "##### Support Vector Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b648dd4",
   "metadata": {},
   "source": [
    "Support Vector Machines create complex decision boundaries used for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5e19a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from graph import points, labels\n",
    "\n",
    "classifier = SVC(kernel = 'linear')\n",
    "classifier.fit(points, labels)\n",
    "print(classifier.predict([[3, 4], [6, 7]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3fd9eed",
   "metadata": {},
   "source": [
    "SVMs try to maximize the size of the margin while still correctly separating the points of each class. As a result, outliers can be a problem.\n",
    "\n",
    "Up to this point, we have been using data sets that are linearly separable. This means that it’s possible to draw a straight decision boundary between the two classes. However, what would happen if an SVM came along a dataset that wasn’t linearly separable?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "421141ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kernel example\n",
    "from sklearn.svm import SVC\n",
    "from graph import points, labels\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "training_data, validation_data, training_labels, validation_labels = train_test_split(points, labels, train_size = 0.8, test_size = 0.2, random_state = 100)\n",
    "\n",
    "classifier = SVC(kernel = \"poly\", degree = 2)\n",
    "classifier.fit(training_data, training_labels)\n",
    "print(classifier.score(validation_data, validation_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4c9e9b",
   "metadata": {},
   "source": [
    "#### Random Forests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ccd66cc",
   "metadata": {},
   "source": [
    "You will learn how to ensemble decision trees -- which are often prone to overfitting -- using random forests algorithm with scitkit-learn.\n",
    "\n",
    "We need to find another way to generalize our decision trees (other than pruning), which is where the concept of a random forest algorithm from."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83900b28",
   "metadata": {},
   "source": [
    "###### Bootstrapping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad5582c",
   "metadata": {},
   "source": [
    "To make a random forest, we use a technique called bagging, which is short for bootstrap aggregating. This exercise will explain bootstrapping, which is a type of sampling method done with replacement.\n",
    "\n",
    "How it works is as follows: every time a decision tree is made, it is created using a different subset of the points in the training set. For example, if our training set had 1000 rows in it, we could make a decision tree by picking 100 of those rows at random to build the tree. This way, every tree is different, but all trees will still be created from a portion of the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbaeb8f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import codecademylib3\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "#Models from scikit learn module:\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "df = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/car/car.data', names=['buying', 'maint', 'doors', 'persons', 'lug_boot', 'safety', 'accep'])\n",
    "df['accep'] = ~(df['accep']=='unacc') #1 is acceptable, 0 if not acceptable\n",
    "X = pd.get_dummies(df.iloc[:,0:6], drop_first=True)\n",
    "y = df['accep']\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X,y, random_state=0, test_size=0.25)\n",
    "nrows = df.shape[0]\n",
    "\n",
    "## 1. Print number of rows and distribution of safety ratings\n",
    "print(nrows)\n",
    "print(f'Distribution of safety ratings in {nrows} of data:')\n",
    "print(df.safety.value_counts(normalize=True))\n",
    "\n",
    "## 2. Create bootstrapped sample\n",
    "boot_sample = df.sample(nrows, replace=True)\n",
    "print(f'Distribution of safety ratings in bootstrapped sample data:')\n",
    "print(boot_sample.safety.value_counts(normalize=True))\n",
    "\n",
    "\n",
    "## 3. Create 1000 bootstrapped samples\n",
    "low_perc = []\n",
    "for i in range(1000):\n",
    "    boot_sample = df.sample(nrows, replace=True)\n",
    "    low_perc.append(boot_sample.safety.value_counts(normalize=True)['low'])\n",
    "\n",
    "## 4. Plot a histogram of the low percentage values\n",
    "mean_lp = np.mean(low_perc) \n",
    "print(mean_lp)\n",
    "plt.hist(low_perc, bins=20);\n",
    "plt.xlabel('Low Percentage')\n",
    "plt.show()\n",
    "\n",
    "## 5. What are the 2.5 and 97.5 percentiles?\n",
    "print(f'Average low percentage: {np.mean(low_perc).round(4)}')\n",
    "\n",
    "low_perc.sort()\n",
    "print(f'95% Confidence Interval for low percengage: ({low_perc[25].round(4)},{low_perc[975].round(4)})')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af333da7",
   "metadata": {},
   "source": [
    "##### Bagging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f43d64c1",
   "metadata": {},
   "source": [
    "Random forests create different trees using a process known as bagging, which is short for bootstrapped aggregating. As we already covered bootstrapping, the process starts with creating a single decision tree on a bootstrapped sample of data points in the training set. Then after many trees have been made, the results are “aggregated” together. \n",
    "\n",
    "In the case of a classification task, often the aggregation is taking the majority vote of the individual classifiers. For regression tasks, often the aggregation is the average of the individual regressors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c768e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import tree\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "df = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/car/car.data', names=['buying', 'maint', 'doors', 'persons', 'lug_boot', 'safety', 'accep'])\n",
    "df['accep'] = ~(df['accep']=='unacc') #1 is acceptable, 0 if not acceptable\n",
    "X = pd.get_dummies(df.iloc[:,0:6], drop_first=True)\n",
    "y = df['accep']\n",
    "x_train, x_test, y_train, y_test = train_test_split(X,y, random_state=0, test_size=0.25)\n",
    "\n",
    "#original decision tree trained on full training set\n",
    "dt = DecisionTreeClassifier(max_depth=5)\n",
    "dt.fit(x_train, y_train)\n",
    "print(f'Accuracy score of DT on test set (trained using full set): {dt.score(x_test, y_test).round(4)}')\n",
    "\n",
    "#2. New decision tree trained on bootstrapped sample\n",
    "dt2 = DecisionTreeClassifier(max_depth=5)\n",
    "#ids are the indices of the bootstrapped sample\n",
    "ids = x_train.sample(x_train.shape[0], replace=True, random_state=0).index\n",
    "dt2.fit(x_train.loc[ids], y_train[ids])#max_depth=50,criterion='gini')\n",
    "print(f'Accuracy score of DT on test set (trained using bootstrapped sample): {dt2.score(x_test, y_test).round(4)}')\n",
    "\n",
    "## 3. Bootstapping ten samples and aggregating the results:\n",
    "preds = []\n",
    "random_state = 0\n",
    "for i in range(10):\n",
    "    ids = x_train.sample(x_train.shape[0], replace=True, random_state=random_state+i).index\n",
    "    dt2.fit(x_train.loc[ids], y_train[ids])\n",
    "    preds.append(dt2.predict(x_test))   \n",
    "ba_pred = np.array(preds).mean(0)\n",
    "\n",
    "# 4. Calculate accuracy of the bagged sample\n",
    "ba_accuracy = accuracy_score(ba_pred>=0.5, y_test)\n",
    "print(f'Accuracy score of aggregated 10 bootstrapped samples:{ba_accuracy.round(4)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d74e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.w3schools.com/python/python_ml_bagging.asp\n",
    "https://www.w3schools.com/python/python_ml_cross_validation.asp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c77b4a2",
   "metadata": {},
   "source": [
    "##### Random Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f1bfec",
   "metadata": {},
   "source": [
    "In addition to using bootstrapped samples of our dataset, we can continue to add variety to the ways our trees are created by randomly selecting the features that are used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2223db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "df = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/car/car.data', names=['buying', 'maint', 'doors', 'persons', 'lug_boot', 'safety', 'accep'])\n",
    "df['accep'] = ~(df['accep']=='unacc') #1 is acceptable, 0 if not acceptable\n",
    "X = pd.get_dummies(df.iloc[:,0:6], drop_first=True)\n",
    "y = df['accep']\n",
    "x_train, x_test, y_train, y_test = train_test_split(X,y, random_state=0, test_size=0.25)\n",
    "dt = DecisionTreeClassifier()\n",
    "dt.fit(x_train, y_train)\n",
    "print(\"Accuracy score of DT on test set (trained using full feature set):\")\n",
    "accuracy_dt = dt.score(x_test, y_test)\n",
    "print(accuracy_dt)\n",
    "\n",
    "# 1. Create rand_features, random samples from the set of features\n",
    "rand_features = np.random.choice(x_train.columns,10)\n",
    "\n",
    "# Make new decision tree trained on random sample of 10 features and calculate the new accuracy score\n",
    "dt2 = DecisionTreeClassifier()\n",
    "\n",
    "dt2.fit(x_train[rand_features], y_train)\n",
    "print(\"Accuracy score of DT on test set (trained using random feature sample):\")\n",
    "accuracy_dt2 = dt2.score(x_test[rand_features], y_test)\n",
    "print(accuracy_dt2)\n",
    "\n",
    "# 2. Build decision trees on 10 different random samples \n",
    "predictions = []\n",
    "for i in range(10):\n",
    "    rand_features = np.random.choice(x_train.columns,10)\n",
    "    dt2.fit(x_train[rand_features], y_train)\n",
    "    predictions.append(dt2.predict(x_test[rand_features]))\n",
    "\n",
    "## 3. Get aggregate predictions and accuracy score\n",
    "prob_predictions = np.array(predictions).mean(0)\n",
    "agg_predictions = (prob_predictions>0.5)\n",
    "agg_accuracy = accuracy_score(agg_predictions, y_test)\n",
    "print('Accuracy score of aggregated 10 samples:')\n",
    "print(agg_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4abb11e8",
   "metadata": {},
   "source": [
    "##### Bagging in `scikit-learn`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9aaa5c6",
   "metadata": {},
   "source": [
    "The two steps we walked through above created trees on bootstrapped samples and randomly selecting features. These can be combined together and implemented at the same time! Combining them adds an additional variation to the base learners for the ensemble model. This in turn increases the ability of the model to generalize to new and unseen data, i.e., it minimizes bias and increases variance. Rather than re-doing this process manually, we will use scikit-learn‘s bagging implementation, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d28aae0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, BaggingClassifier, RandomForestRegressor\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "df = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/car/car.data', names=['buying', 'maint', 'doors', 'persons', 'lug_boot', 'safety', 'accep'])\n",
    "df['accep'] = ~(df['accep']=='unacc') #1 is acceptable, 0 if not acceptable\n",
    "X = pd.get_dummies(df.iloc[:,0:6], drop_first=True)\n",
    "y = df['accep']\n",
    "x_train, x_test, y_train, y_test = train_test_split(X,y, random_state=0, test_size=0.25)\n",
    "\n",
    "# 1. Bagging classifier with 10 Decision Tree base estimators\n",
    "bag_dt = BaggingClassifier(base_estimator=DecisionTreeClassifier(max_depth=5), n_estimators=10)\n",
    "bag_dt.fit(x_train, y_train)\n",
    "\n",
    "print('Accuracy score of Bagged Classifier, 10 estimators:')\n",
    "bag_accuracy = bag_dt.score(x_test, y_test)\n",
    "print(bag_accuracy)\n",
    "\n",
    "# 2.Set `max_features` to 10.\n",
    "bag_dt_10 = BaggingClassifier(base_estimator=DecisionTreeClassifier(max_depth=5), n_estimators=10, max_features=10)\n",
    "bag_dt_10.fit(x_train, y_train)\n",
    "\n",
    "print('Accuracy score of Bagged Classifier, 10 estimators, 10 max features:')\n",
    "bag_accuracy_10 = bag_dt_10.score(x_test, y_test)\n",
    "print(bag_accuracy_10)\n",
    "\n",
    "\n",
    "# 3. Change base estimator to Logistic Regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "bag_lr = BaggingClassifier(base_estimator=LogisticRegression(),\n",
    "                         n_estimators=10, max_features=10)\n",
    "bag_lr.fit(x_train, y_train)\n",
    "\n",
    "print('Accuracy score of Logistic Regression, 10 estimators:')\n",
    "bag_accuracy_lr = bag_lr.score(x_test, y_test)\n",
    "print(bag_accuracy_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9caedab",
   "metadata": {},
   "source": [
    "##### Train and Predict using `scikit-learn`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d5b5dc",
   "metadata": {},
   "source": [
    "Now that we have covered two major ways to combine trees, both in terms of samples and features, we are ready to get to the implementation of random forests! This will be similar to what we covered in the previous exercises, but the random forest algorithm has a slightly different way of randomly choosing features. Rather than choosing a single random set at the onset, each split chooses a different random set.\n",
    "\n",
    "One question to consider is how to choose the number of features to randomly select. Why did we choose 3 in this example? A good rule of thumb is select as many features as the square root of the total number of features. Our car dataset doesn’t have a lot of features, so in this example, it’s difficult to follow this rule. But if we had a dataset with 25 features, we’d want to randomly select 5 features to consider at every split point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c7198cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "\n",
    "df = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/car/car.data', names=['buying', 'maint', 'doors', 'persons', 'lug_boot', 'safety', 'accep'])\n",
    "df['accep'] = ~(df['accep']=='unacc') #1 is acceptable, 0 if not acceptable\n",
    "X = pd.get_dummies(df.iloc[:,0:6], drop_first=True)\n",
    "y = df['accep']\n",
    "x_train, x_test, y_train, y_test = train_test_split(X,y, random_state=0, test_size=0.25)\n",
    "\n",
    "# 1. Create a Random Forest Classifier and print its parameters\n",
    "rf = RandomForestClassifier()\n",
    "print('Random Forest parameters:')\n",
    "rf_params = rf.get_params()\n",
    "print(rf_params)\n",
    "\n",
    "# 2. Fit the Random Forest Classifier to training data and calculate accuracy score on the test data\n",
    "rf.fit(x_train, y_train)\n",
    "y_pred = rf.predict(x_test)\n",
    "print('Test set accuracy:')\n",
    "rf_accuracy = rf.score(x_test, y_test)\n",
    "print(rf_accuracy)\n",
    "\n",
    "# 3. Calculate Precision and Recall scores and the Confusion Matrix\n",
    "rf_precision = precision_score(y_test, y_pred)\n",
    "print(f'Test set precision: {rf_precision}')\n",
    "rf_recall = recall_score(y_test, y_pred)\n",
    "print(f'Test set recall: {rf_recall}')\n",
    "rf_confusion_matrix = confusion_matrix(y_test, y_pred)\n",
    "print(f'Test set confusion matrix:\\n{rf_confusion_matrix}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e8b12ff",
   "metadata": {},
   "source": [
    "##### Random Forest Regressor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8805e8b6",
   "metadata": {},
   "source": [
    "Just like in decision trees, we can use random forests for regression as well! It is important to know when to use regression or classification — this usually comes down to what type of variable your target is. Previously, we were using a binary categorical variable (acceptable versus not), so a classification model was used.\n",
    "\n",
    "Now, instead of a classification task, we will use scikit-learn‘s RandomForestRegressor() to carry out a regression task. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843ab9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "\n",
    "df = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/car/car.data', names=['buying', 'maint', 'doors', 'persons', 'lug_boot', 'safety', 'accep'])\n",
    "df['accep'] = ~(df['accep']=='unacc') #1 is acceptable, 0 if not acceptable\n",
    "X = pd.get_dummies(df.iloc[:,0:6], drop_first=True)\n",
    "\n",
    "## Generating some fake prices for regression! :) \n",
    "fake_prices = (15000 + 25*df.index.values)+np.random.normal(size=df.shape[0])*5000\n",
    "df['price'] = fake_prices\n",
    "print(df.price.describe())\n",
    "y = df['price']\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X,y, random_state=0, test_size=0.25)\n",
    "\n",
    "# 1. Create a Random Regressor and print `R^2` scores on training and test data\n",
    "rfr = RandomForestRegressor()\n",
    "rfr.fit(x_train, y_train)\n",
    "\n",
    "r_squared_train = rfr.score(x_train, y_train)\n",
    "print(f'Train set R^2: {r_squared_train}')\n",
    "\n",
    "r_squared_test = rfr.score(x_test, y_test)\n",
    "print(f'Test set R^2: {r_squared_test}')\n",
    "\n",
    "# 2. Print Mean Absolute Error on training and test data\n",
    "\n",
    "avg_price = y.mean()\n",
    "print(f'Avg Price Train/Test: {avg_price}')\n",
    "\n",
    "y_pred_train =rfr.predict(x_train)\n",
    "y_pred_test =rfr.predict(x_test)\n",
    "\n",
    "mae_train = mean_absolute_error(y_train, y_pred_train)\n",
    "print(f'Train set MAE: {mae_train}')\n",
    "\n",
    "mae_test = mean_absolute_error(y_test, y_pred_test)\n",
    "print(f'Test set MAE: {mae_test}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb8bfe41",
   "metadata": {},
   "source": [
    "    A random forest is an ensemble machine learning model. It makes a classification by aggregating the classifications of many decision trees.\n",
    "    \n",
    "    Random forests are used to avoid overfitting. By aggregating the classification of multiple trees, having overfitted trees in a random forest is less impactful.\n",
    "    \n",
    "    Every decision tree in a random forest is created by using a different subset of data points from the training set. Those data points are chosen at random with replacement, which means a single data point can be chosen more than once. This process is known as bagging.\n",
    "    \n",
    "    When creating a tree in a random forest, a randomly selected subset of features are considered as candidates for the best splitting feature. If your dataset has n features, it is common practice to randomly select the square root of n features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b2f53e3",
   "metadata": {},
   "source": [
    "### Bayes' Theorem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "895b907e",
   "metadata": {},
   "source": [
    "Bayes’ Theorem is the basis of a branch of statistics called Bayesian Statistics, where we take prior knowledge into account before calculating new probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1690176d",
   "metadata": {},
   "source": [
    "Suppose you are a doctor and you need to test if a patient has a certain rare disease. The test is very accurate: it’s correct 99% of the time. The disease is very rare: only 1 in 100,000 patients have it.\n",
    "\n",
    "You administer the test and it comes back positive, so your patient must have the disease, right? Not necessarily. If we just consider the test, there is only a 1% chance that it is wrong, but we actually have more information: we know how rare the disease is.Given that the test came back positive, there are two possibilities:\n",
    "\n",
    "    The patient had the disease, and the test correctly diagnosed the disease.\n",
    "    The patient didn’t have the disease and the test incorrectly diagnosed that they had the disease."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "223249c3",
   "metadata": {},
   "source": [
    "##### The Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d0f408",
   "metadata": {},
   "source": [
    "A Naive Bayes classifier is a supervised machine learning algorithm that leverages Bayes’ Theorem to make predictions and classifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d586196f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from reviews import baby_counter, baby_training, instant_video_counter, instant_video_training, video_game_counter, video_game_training\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "review = \"this game was violent\"\n",
    "\n",
    "baby_review_counts = baby_counter.transform([review])\n",
    "instant_video_review_counts = instant_video_counter.transform([review])\n",
    "video_game_review_counts = video_game_counter.transform([review])\n",
    "\n",
    "baby_classifier = MultinomialNB()\n",
    "instant_video_classifier = MultinomialNB()\n",
    "video_game_classifier = MultinomialNB()\n",
    "\n",
    "baby_labels = [0] * 1000 + [1] * 1000\n",
    "instant_video_labels = [0] * 1000 + [1] * 1000\n",
    "video_game_labels = [0] * 1000 + [1] * 1000\n",
    "\n",
    "\n",
    "baby_classifier.fit(baby_training, baby_labels)\n",
    "instant_video_classifier.fit(instant_video_training, instant_video_labels)\n",
    "video_game_classifier.fit(video_game_training, video_game_labels)\n",
    "\n",
    "print(\"Baby training set: \" +str(baby_classifier.predict_proba(baby_review_counts)))\n",
    "print(\"Amazon Instant Video training set: \" + str(instant_video_classifier.predict_proba(instant_video_review_counts)))\n",
    "print(\"Video Games training set: \" + str(video_game_classifier.predict_proba(video_game_review_counts)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b504a5",
   "metadata": {},
   "source": [
    "## Unsupervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a845cf",
   "metadata": {},
   "source": [
    "Unsupervised Learning is a type of machine learning where the program learns the inherent structure of the data based on unlabeled examples.\n",
    "\n",
    "Clustering is a common unsupervised machine learning approach that finds patterns and structures in unlabeled data by grouping them into clusters.\n",
    "\n",
    "Some examples:\n",
    "\n",
    "    Social networks clustering topics in their news feed\n",
    "    Consumer sites clustering users for recommendations\n",
    "    Search engines to group similar objects in one cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f42664b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example  \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "from plot import plot_clusters\n",
    "\n",
    "# Load the data\n",
    "media_usage = pd.read_csv('media_usage.csv')\n",
    "\n",
    "# Create the model\n",
    "kmeans = KMeans(n_clusters=3)\n",
    "\n",
    "# Fit the model to the data\n",
    "kmeans.fit(media_usage)\n",
    "\n",
    "labels = kmeans.predict(media_usage)\n",
    "\n",
    "# Plot the clusters\n",
    "plot_clusters(media_usage, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d29ade0",
   "metadata": {},
   "source": [
    "A social media platform wants to separate their users into categories based on what kind of content they engage with. They have collected three pieces of data from a sample of users:\n",
    "\n",
    "    Number of hours per week spent reading posts\n",
    "    Number of hours per week spent watching videos\n",
    "    Number of hours per week spent in virtual reality\n",
    "\n",
    "The company is using an algorithm called k-means clustering to sort users into three different groups."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab94bd4",
   "metadata": {},
   "source": [
    "Often, the data you encounter in the real world won’t be sorted into categories and won’t have labeled answers to your question. Finding patterns in this type of data, unlabeled data, is a common theme in many machine learning applications. Unsupervised Learning is how we find patterns and structure in these data.\n",
    "\n",
    "Clustering is the most well-known unsupervised learning technique. It finds structure in unlabeled data by identifying similar groups, or clusters. Examples of clustering applications are:\n",
    "\n",
    "    Recommendation engines: group products to personalize the user experience\n",
    "    Search engines: group news topics and search results\n",
    "    Market segmentation: group customers based on geography, demography, and behaviors\n",
    "    Image segmentation: medical imaging or road scene segmentation on self-driving cars\n",
    "    Text clustering: group similar texts together based on word usage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c1ac4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.w3schools.com/python/python_ml_hierarchial_clustering.asp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef597fc1",
   "metadata": {},
   "source": [
    "#### K-Means Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc74570",
   "metadata": {},
   "source": [
    "The goal of clustering is to separate data so that data similar to one another are in the same group, while data different from one another are in different groups. So two questions arise:\n",
    "\n",
    "    How many groups do we choose?\n",
    "    How do we define similarity?\n",
    "\n",
    "k-means is the most popular and well-known clustering algorithm, and it tries to address these two questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8842ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "samples = iris.data\n",
    "\n",
    "model = KMeans(n_clusters=3)\n",
    "\n",
    "model.fit(samples)\n",
    "\n",
    "labels = model.predict(samples)\n",
    "\n",
    "print(labels)\n",
    "\n",
    "# Make a scatter plot of x and y and using labels to define the colors\n",
    "x = samples[:,0]\n",
    "y = samples[:,1]\n",
    "\n",
    "plt.scatter(x, y, c=labels, alpha=0.5)\n",
    "\n",
    "plt.xlabel('sepal length (cm)')\n",
    "plt.ylabel('sepal width (cm)')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7960b946",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.w3schools.com/python/python_ml_k-means.asp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c0c687",
   "metadata": {},
   "source": [
    "#### Principal Component Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27db1b77",
   "metadata": {},
   "source": [
    "The motivation of Principal Component Analysis (PCA) is to find a new set of features that are ordered by the amount of variation (and therefore, information) they contain. We can then select a subset of these PCA features. This leaves us with lower-dimensional data that still retains most of the information contained in the larger dataset. \n",
    "\n",
    "We will begin by taking a look at the features that describe different categories of an object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a1da33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "import codecademylib3\n",
    "\n",
    "data_matrix = pd.read_csv('./data_matrix.csv')\n",
    "\n",
    "# 1. Standardize the data matrix\n",
    "mean = data_matrix.mean(axis=0)\n",
    "sttd = data_matrix.std(axis=0)\n",
    "data_matrix_standardized = (data_matrix - mean) / sttd\n",
    "print(data_matrix_standardized.head())\n",
    "\n",
    "# 2. Find the principal components\n",
    "pca = PCA()\n",
    "components = pca.fit(data_matrix_standardized).components_\n",
    "components = pd.DataFrame(components).transpose()\n",
    "components.index =  data_matrix.columns\n",
    "print(components)\n",
    "\n",
    "# 3. Calculate the variance/info ratios\n",
    "var_ratio = pca.explained_variance_ratio_\n",
    "var_ratio= pd.DataFrame(var_ratio).transpose()\n",
    "print(var_ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ac7ee3",
   "metadata": {},
   "source": [
    "In this checkpoint, we will be using the first four principal components as our training data for a Support Vector Classifier (SVC). We will compare this to a model fit with the entire dataset (16 features) using the average likelihood score. Average likelihood is a model evaluation metric; the higher the average likelihood, the better the fit.\n",
    "Instructions\n",
    "Checkpoint 1 Passed\n",
    "\n",
    "Read through the code to make sure that you understand what’s happening. Here are the steps:\n",
    "\n",
    "    Transform the original data by projecting it onto the first four principal axes. We chose four PCs because we previously found that they contain 95% of the variance in the original data\n",
    "    Split the data into 67% training and 33% testing sets\n",
    "    Use the transformed training data to fit an SVM model\n",
    "    Print out the average likelihood score for the testing data\n",
    "    Re-split the original 16 standardized features into training and test sets\n",
    "    Fit the same SVM model on the training set with all 16 features\n",
    "    Print out the average likelihood score for the test data\n",
    "\n",
    "Notice that the score for the model using the first 4 principal components is higher than for the model that was fit with the 16 original features. We only needed 1/4 of the data to get even better model performance!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dade2f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import train_test_split\n",
    " \n",
    " \n",
    "data_matrix_standardized = pd.read_csv('./data_matrix_standardized.csv')\n",
    "classes = pd.read_csv('./classes.csv')\n",
    " \n",
    "# We will use the classes as y\n",
    "y = classes.Class.astype('category').cat.codes\n",
    " \n",
    "# Get principal components with 4 features and save as X\n",
    "pca_1 = PCA(n_components=4) \n",
    "X = pca_1.fit_transform(data_matrix_standardized) \n",
    " \n",
    "# Split the data into 33% testing and the rest training\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    " \n",
    "# Create a Linear Support Vector Classifier\n",
    "svc_1 = LinearSVC(random_state=0, tol=1e-5)\n",
    "svc_1.fit(X_train, y_train) \n",
    " \n",
    "# Generate a score for the testing data\n",
    "score_1 = svc_1.score(X_test, y_test)\n",
    "print(f'Score for model with 4 PCA features: {score_1}')\n",
    " \n",
    "# Split the original data intro 33% testing and the rest training\n",
    "X_train, X_test, y_train, y_test = train_test_split(data_matrix_standardized, y, test_size=0.33, random_state=42)\n",
    " \n",
    "# Create a Linear Support Vector Classifier\n",
    "svc_2 = LinearSVC(random_state=0)\n",
    "svc_2.fit(X_train, y_train)\n",
    " \n",
    "# Generate a score for the testing data\n",
    "score_2 = svc_2.score(X_test, y_test)\n",
    "print(f'Score for model with original features: {score_2}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
